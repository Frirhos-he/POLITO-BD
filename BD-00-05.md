
# Introduction to Big Data

Links:
BigData@Polito cluster
▪ https://jupyter.polito.it
▪ https://hue.polito.it

Course link  https://dbdmg.polito.it/dbdmg_web/index.php/2023/09/27/big-data-architectures-and-data-analytics-2023-2024/


## Written exam
▪ 2 programming exercises (max 27 points)

▪ Design and develop Java programs based on the Hadoop 
MapReduce programming paradigm and/or Spark RDDs

▪ 2 questions / theoretical exercises (max 4 points)

▪ 90 minutes

▪ The exam is open book

## INTRO

▪Big data: Data whose scale, diversity and complexity require new architectures, techniques, algorithms and analytics to manage it and extract value and  hidden knowledge from it.

▪The Vs of big data: 

    Volume: scale of data

    Variety: different forms of data

    Velocity: analysis of streaming data

    Veracity: uncertainty of data

    Value: exploit information provided by data

▪BD value chain: 

    generation(passive/active/automatic)
    
    acquisition(collection/trasmission/preprocessing)
    
    storage(infrastructure, data managment, programming model)
    
    analysis

▪Bottleneck --> Solution:
    
    needs to transfer data from HD to processor to process data --> Transfer the processing power to the data (on multiple distributed disks).

## Big data architectures

▪ A big data architecture is designed to handle the ingestion, processing, and analysis of data that is too large or complex for traditional database systems.

### Big data solutions

    ▪ Batch processing of big data sources at rest

    ▪ Real-time processing of big data in motion

    ▪ Interactive exploration of big data

    ▪ Predictive analytics and machine learning

The mostly used is Lambda Architecture.

### When to consider big data architectures

    Store and process data in volumes too large for a traditional database
    
    Transform unstructured data for analysis and reporting
    
    Capture, process, and analyze unbounded streams of data in real time, or with low latency

## Lambda architecture
Lambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods.This approach to architecture attempts to balance latency, throughput, and fault tolerance by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data.

### Queries - Properties

    -  Latency: The time it takes to run a query
    -  Timeliness: How up to date the query results are (freshness and consistency)
    -  Accuracy: Tradeoff between performance and scalability (approximations)

### LA Requirments
    - Fault-tolerant against both hardware failures and human errors
    - Support variety of use cases that include low latency querying as well as updates
    - Linear scale-out capabilities
    - Extensible, so that the system is manageable and can accommodate newer features easily

### LA Paths
    - Hot(speed layer): analyzes data in real time ; designed for low latency, at the expense of accuracy
    - Cold(batch layer): stores incoming data in raw, performs batch processing , stores as batch views (on serving layer).

### Layers
    Batch layer functions

        (i) managing the master dataset(an immutable, append-only set of raw data)

        (ii) to pre-compute the batch views

    Serving layer: it indexes the batch views so that they can be queried in low-latency   
    
    Speed layer deals with recent data only
 
    Incoming query can be answered by merging results from batch views and realtime views


# Hadoop
## Hadoop and Map reduce

Sources of failures

    - Hardware/Software
    
    - Electrical, Cooling
    
    - Unavailability of a resource due to overload

Types of failure

    - Permanent

    - Transient

Network becomes the bottleneck: data should be moved if indispensable instead move code and computation to data (data locality).

Architectures
    - Single node: (small data) loaded in main memory, (large data) load a chunk at a time 
    
    - Cluster: standard in BD context, data/computation distribuited across server linked through switch. Challenges: 
        - DP (problem decomposition, parallelization, scheduling)
        - task scheduling (speed up, exploit, reduce impact on node failure)
        - D data storage (Redundancy)
        - network bottleneck

Types of scalability

    - Vertical: add more resource/power to single node

    - Horizontal: add more nodes to system. Preferred in BD applications 


## Hadoop
Scalable fault-tolerant distributed system for BD. Designed for data intensive workloads
HPC: a supercomputer with a high-level computational capacity. Designed for CPU intensive tasks. (small dataset)

### Main components
    - Distributed Big Data Processing Infrastructure: based on the MapReduce programming paradigm
        ▪ Provides a high-level abstraction view of the distribuited part (no need to care about task scheduling and syncro)
        ▪ Fault-tolerant
    - HDFS: fault tolerant, high availability distribuited storage, provides global file namespace. 
    Usage pattern: huge files, data rarely updated, common reads and appends.
    File: split in chunks across servers (different racks if possible). 64-128 MB. Master node stores HDFS metadata, might be replicated, is accessed by HDFS APIs.

## MapReduce
Designed for batch processing involving (mostly) full scans of the input data. 

It does not feet well

   - Iterative problems
   - Recursive problems
   - Stream data processing
   - Real-time processing

### MapReduce Programming Paradigm (based on Functional programming)

- Map function: It is applied over each element of an input data set and emits a set of (key, value) pairs

- Reduce function: It is applied over each set of (key, value) pairs (emitted by the map function) with the same key and emits a set of (key, value) pairs -> Final result

Phases: 

    - map: invoked one time per input element (parallelizable isolated transformation)

    - shuffle and sort: group the output of the map phase by key
    
    - reduce phase: invoked one time for each distinct key and aggregates all the values associated with it (parallelizable and isolated)

Data structure: Key-value pair, both I/O of map reduce, and map are lists of KVP.

## HDFS commands

hdfs dfs -ls folder

      List the content of a folder of the HDFS file system

hdfs dfs -cat file

       Show the content of a file of the HDFS file system

hdfs dfs -put local_file HDFS_path

       Copy a file from the local file system to the HDFS file system

hdfs dfs -get HDFS_path local_file

       Copy a file from the HDFS file system to the local file system

hdfs dfs -rm HDFS_path

        Delete a file from the HDFS file system

hadoop jar MyApplication.jar it.polito.bigdata.hadoop.DriverMyApplication 1 inputdatafolder/ outputdatafolder/

         The following command executes/submits a MapReduce application

## MapReduce Program
    - Driver class: containing the method/code that coordinates the configuration of the job and the “workflow” of the application, obj runs on the client machine
    - Mapper class: obj runs on cluster, invoked one time for each input (key, value) pair
    - Reducer class: obj runs on cluster, invoked one time for each distinct key

Job: Execution/run of a MapReduce code over a data set

Task: Execution/run of a Mapper (Map task) or a Reducer (Reduce task) on a slice of data

Input split: Fixed-size piece of the input data

Cluser: refers to a group of interconnected computers or servers that work together to perform tasks as a single system

Phases:
    - map: invoked one time for each input (key, value) pair, emits a sets of intermediate key-value pairs stored in local file system of computing server.
    - shuffle and sort: intermediate results are aggregated and a set of (key,[]) pairs is generated, one per key.
    - reduce: invoked one time per (key,[]) pairs emits a set of (key, value) pairs stored in HDFS.

Parallelization is reached instantiating one mapper per input split and N reducers, one reducer is associated with set of keys.

Mappers/Reducers are executed on node of the clusters.

### Driver class
    run(…) method
    - Configures the job (name, job I/O, mapper class I/O, reducer class I/O #)

### Mapper class
    map(…) method
    - Processes its input (key, value) pairs, Emits (key, value) pairs

### Reducer class
    reduce(…) method
    - Processes its input (key, [list of values]) pairs, Emits (key, value) pairs

### Data types (opt for network serialization)
    - org.apache.hadoop.io.Text
    - org.apache.hadoop.io.IntWritable
    - org.apache.hadoop.io.LongWritable
    - org.apache.hadoop.io.FloatWritable

Implement the org.apache.hadoop.io.Writable (represent values) and org.apache.hadoop.io.WritableComparable (comparable for sort/shyffle) interfaces

### InputFormat
Input of the MapReduce program is an HDFS folder. 

InputFormat read input data, validate input file with input format, split input file into input splits (each assigned to a mapper), provide the recordReader to divide logical input split in (key, value) pairs for mapper. 

<b>Standard inputs file</b>

    - TextInputFormat: key = offset, value = content of line

    - KeyValueTextInputFormat: key <separator> value

### OutputFormat
Output of the MapReduce program is an HDFS folder

<b>Standard output file</b>

    - TextOutputFormat: key <separator> value

## Combiner

(key,value) pairs emitted by the Mappers are sent to the Reducers through the network “pre-aggregations” (on local disk) could be performed to limit the amount of network data. Only if the reduce function is commutative and associative. MR job should not depend on Combiner exe.

Processes (key, [list of values]) pairs and emits (key, value) pairs.

Runs on the cluster.

When the reducer and the combiner classes perform the same computation (the reduce method of the two classes is the same)
We can simply specify that reducer class is also the combiner class.

▪ In the driver
job.setCombinerClass(WordCountReducer.class);
