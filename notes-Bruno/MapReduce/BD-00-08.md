
# Introduction to Big Data

Links:
BigData@Polito cluster
▪ https://jupyter.polito.it
▪ https://hue.polito.it

Course link  https://dbdmg.polito.it/dbdmg_web/index.php/2023/09/27/big-data-architectures-and-data-analytics-2023-2024/


## Written exam
▪ 2 programming exercises (max 27 points)

▪ Design and develop Java programs based on the Hadoop 
MapReduce programming paradigm and/or Spark RDDs

▪ 2 questions / theoretical exercises (max 4 points)

▪ 90 minutes

▪ The exam is open book

## INTRO

▪Big data: Data whose scale, diversity and complexity require new architectures, techniques, algorithms and analytics to manage it and extract value and  hidden knowledge from it.

▪The Vs of big data: 

    Volume: scale of data

    Variety: different forms of data

    Velocity: analysis of streaming data

    Veracity: uncertainty of data

    Value: exploit information provided by data

▪BD value chain: 

    generation(passive/active/automatic)
    
    acquisition(collection/trasmission/preprocessing)
    
    storage(infrastructure, data managment, programming model)
    
    analysis

▪Bottleneck --> Solution:
    
    needs to transfer data from HD to processor to process data --> Transfer the processing power to the data (on multiple distributed disks).

## Big data architectures

▪ A big data architecture is designed to handle the ingestion, processing, and analysis of data that is too large or complex for traditional database systems.

### Big data solutions

    ▪ Batch processing of big data sources at rest

    ▪ Real-time processing of big data in motion

    ▪ Interactive exploration of big data

    ▪ Predictive analytics and machine learning

The mostly used is Lambda Architecture.

### When to consider big data architectures

    Store and process data in volumes too large for a traditional database
    
    Transform unstructured data for analysis and reporting
    
    Capture, process, and analyze unbounded streams of data in real time, or with low latency

## Lambda architecture
Lambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods.This approach to architecture attempts to balance latency, throughput, and fault tolerance by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data.

### Queries - Properties

    -  Latency: The time it takes to run a query
    -  Timeliness: How up to date the query results are (freshness and consistency)
    -  Accuracy: Tradeoff between performance and scalability (approximations)

### LA Requirments
    - Fault-tolerant against both hardware failures and human errors
    - Support variety of use cases that include low latency querying as well as updates
    - Linear scale-out capabilities
    - Extensible, so that the system is manageable and can accommodate newer features easily

### LA Paths
    - Hot(speed layer): analyzes data in real time ; designed for low latency, at the expense of accuracy
    - Cold(batch layer): stores incoming data in raw, performs batch processing , stores as batch views (on serving layer).

### Layers
    Batch layer functions

        (i) managing the master dataset(an immutable, append-only set of raw data)

        (ii) to pre-compute the batch views

    Serving layer: it indexes the batch views so that they can be queried in low-latency   
    
    Speed layer deals with recent data only
 
    Incoming query can be answered by merging results from batch views and realtime views


# Hadoop
## Hadoop and Map reduce

Sources of failures

    - Hardware/Software
    
    - Electrical, Cooling
    
    - Unavailability of a resource due to overload

Types of failure

    - Permanent

    - Transient

Network becomes the bottleneck: data should be moved if indispensable instead move code and computation to data (data locality).

Architectures
    - Single node: (small data) loaded in main memory, (large data) load a chunk at a time 
    
    - Cluster: standard in BD context, data/computation distribuited across server linked through switch. Challenges: 
        - DP (problem decomposition, parallelization, scheduling)
        - task scheduling (speed up, exploit, reduce impact on node failure)
        - D data storage (Redundancy)
        - network bottleneck

Types of scalability

    - Vertical: add more resource/power to single node

    - Horizontal: add more nodes to system. Preferred in BD applications 


## Hadoop
Scalable fault-tolerant distributed system for BD. Designed for data intensive workloads
HPC: a supercomputer with a high-level computational capacity. Designed for CPU intensive tasks. (small dataset)

### Main components
    - Distributed Big Data Processing Infrastructure: based on the MapReduce programming paradigm
        ▪ Provides a high-level abstraction view of the distribuited part (no need to care about task scheduling and syncro)
        ▪ Fault-tolerant
    - HDFS: fault tolerant, high availability distribuited storage, provides global file namespace. 
    Usage pattern: huge files, data rarely updated, common reads and appends.
    File: split in chunks across servers (different racks if possible). 64-128 MB. Master node stores HDFS metadata, might be replicated, is accessed by HDFS APIs.

## MapReduce
Designed for batch processing involving (mostly) full scans of the input data. 

It does not feet well

   - Iterative problems
   - Recursive problems
   - Stream data processing
   - Real-time processing

### MapReduce Programming Paradigm (based on Functional programming)

- Map function: It is applied over each element of an input data set and emits a set of (key, value) pairs

- Reduce function: It is applied over each set of (key, value) pairs (emitted by the map function) with the same key and emits a set of (key, value) pairs -> Final result

Phases: 

    - map: invoked one time per input element (parallelizable isolated transformation)

    - shuffle and sort: group the output of the map phase by key
    
    - reduce phase: invoked one time for each distinct key and aggregates all the values associated with it (parallelizable and isolated)

Data structure: Key-value pair, both I/O of map reduce, and map are lists of KVP.

## HDFS commands

hdfs dfs -ls folder

      List the content of a folder of the HDFS file system

hdfs dfs -cat file

       Show the content of a file of the HDFS file system

hdfs dfs -put local_file HDFS_path

       Copy a file from the local file system to the HDFS file system

hdfs dfs -get HDFS_path local_file

       Copy a file from the HDFS file system to the local file system

hdfs dfs -rm HDFS_path

        Delete a file from the HDFS file system

hadoop jar MyApplication.jar it.polito.bigdata.hadoop.DriverMyApplication 1 inputdatafolder/ outputdatafolder/

         The following command executes/submits a MapReduce application

## MapReduce Program
    - Driver class: containing the method/code that coordinates the configuration of the job and the “workflow” of the application, obj runs on the client machine
    - Mapper class: obj runs on cluster, invoked one time for each input (key, value) pair
    - Reducer class: obj runs on cluster, invoked one time for each distinct key

Job: Execution/run of a MapReduce code over a data set

Task: Execution/run of a Mapper (Map task) or a Reducer (Reduce task) on a slice of data

Input split: Fixed-size piece of the input data

Cluser: refers to a group of interconnected computers or servers that work together to perform tasks as a single system

Phases:
    - map: invoked one time for each input (key, value) pair, emits a sets of intermediate key-value pairs stored in local file system of computing server.
    - shuffle and sort: intermediate results are aggregated and a set of (key,[]) pairs is generated, one per key.
    - reduce: invoked one time per (key,[]) pairs emits a set of (key, value) pairs stored in HDFS.

Parallelization is reached instantiating one mapper per input split and N reducers, one reducer is associated with set of keys.

Mappers/Reducers are executed on node of the clusters.

### Driver class
    run(…) method
    - Configures the job (name, job I/O, mapper class I/O, reducer class I/O #)

### Mapper class
    map(…) method
    - Processes its input (key, value) pairs, Emits (key, value) pairs

### Reducer class
    reduce(…) method
    - Processes its input (key, [list of values]) pairs, Emits (key, value) pairs

### Data types (opt for network serialization)
    - org.apache.hadoop.io.Text
    - org.apache.hadoop.io.IntWritable
    - org.apache.hadoop.io.LongWritable
    - org.apache.hadoop.io.FloatWritable

Implement the org.apache.hadoop.io.Writable (represent values) and org.apache.hadoop.io.WritableComparable (comparable for sort/shyffle) interfaces

### InputFormat
Input of the MapReduce program is an HDFS folder. 

InputFormat read input data, validate input file with input format, split input file into input splits (each assigned to a mapper), provide the recordReader to divide logical input split in (key, value) pairs for mapper. 

<b>Standard inputs file</b>

    - TextInputFormat: key = offset, value = content of line

    - KeyValueTextInputFormat: key <separator> value

### OutputFormat
Output of the MapReduce program is an HDFS folder

<b>Standard output file</b>

    - TextOutputFormat: key <separator> value

## Combiner

(key,value) pairs emitted by the Mappers are sent to the Reducers through the network “pre-aggregations” (on local disk) could be performed to limit the amount of network data. Only if the reduce function is commutative and associative. MR job should not depend on Combiner exe.

Processes (key, [list of values]) pairs and emits (key, value) pairs.

Runs on the cluster.

When the reducer and the combiner classes perform the same computation (the reduce method of the two classes is the same)
We can simply specify that reducer class is also the combiner class.

▪ In the driver job.setCombinerClass(WordCountReducer.class);

## Personalized Data Types

implementing the org.apache.hadoop.io.Writable interface

    ▪ public void readFields(DataInput in)

    ▪ public void write(DataOutput out)

method is “redefined” 

    ▪ public String toString()

implementing the org.apache.hadoop.io.WritableComparable interface
    
    ▪ Implement the compareTo() method

    ▪ Implement the hashCode() method


## Sharing parameters

(property-name, property-value) values cannot be modified by mappers and reducers

    conf.set("property-name", "value");

    context.getConfiguration().get("property-name")

## Counter

Defined by means of Java enum. Incremented in the Mappers and Reducers. Final value available at the end of the job

used by the Mapper and Reducer

    context.getCounter(countername).increment(value);

used by the Driver to retrieve the final values of the counters

    getCounters()
    
    findCounter() 

Ex.
    public static enum COUNTERS {
        ERROR_COUNT,
        MISSING_FIELDS_RECORD_COUNT
    }

    Mapper/Reducer

    context.getCounter(COUNTERS.ERROR_COUNT).increment(1);

    Driver

    Counter errorCounter = job.getCounters().findCounter(COUNTERS.ERROR_COUNT);


## Map-only job

job.setNumReduceTasks(0);

## In-mapper combiner

The setup method is called once for each mapper prior to the many calls of the map method. Used to set the values of in-mapper variables.

The cleanup method is called once for each mapper after the many calls to the map method. Used to emit (key,value) pairs based on the values of the in-mapper variables/statistics

Same about the Reducer.

The in-mapper variables are used to perform the work of the combiner in the mapper. Should be small

class MAPPER
    
    method setup
    
    A ← new AssociativeArray

    method map(offset key, line l)

    for all word w ∈ line l do

    A{w} ←A{w} + 1
    
    method cleanup

    for all word w ∈ A do
    
    EMIT(term w , count A{w})


## Map-Reduce Patterns 

### Summarization pattern

Produce top-level/summarized view of the data
 
    Numerical Summarization: calculate a numerical aggregate (average, max, min, standard deviation) per group

        ▪ Word count
        
        ▪ Record count (per group)
        
        ▪ Min/Max/Count (per group)
        
        ▪ Average/Median/Standard deviation (per group)

    Inverted Index: map terms to a list of identifiers. Word – List of URLs (Inverted Index)

    Counting with counters (Map-Only Job)
    
        ▪ Count number of records

        ▪ Count a small number of unique instances

        ▪ Summarizations

### Filtering Pattern

    ▪ Filtering (map-only job): key is the primary key --> (key, value)

        ▪ Record filtering 
        
        ▪ Tracking events 
        
        ▪ Distributed grep 
        
        ▪ Data cleaning

    ▪ Top K: Each mapper initializes an in-mapper (local) top k list (Main memory). Mapper: key is null, value is in-mapper top k record. Records must be unique otherwise a filtering map-reduce must be performed. Reducer: one.

        ▪ Outlier analysis (based on a ranking function)

        ▪ Select interesting data (based on a ranking function)

    ▪ Distinct : Mapper: key = input record, value = null. Reducer: key = input record, value = null.

        ▪ Duplicate data removal

        ▪ Distinct value selection

### Data organization pattern

    ▪ Binning (Map only): organize I records in categories. For each input (key, value) pair, select the output bin/file associated with it and emit a (key,value) in that file.  No combiner or reducer is used in this pattern    

    
    ▪ Shuffling: Randomize the order of the data.

    Mapper

        ▪ key is a random key (i.e., a random number)

        ▪ value is the input record

    Reducer

        Emit one (key, value) pair for each value in [list-of values] of the input (key, [list-of-values]) pair. Key = value value = null

### Metapatterns

    To organize the workflow of a complex application executing many jobs (phase) (Job Chaining)

### Join pattern

    To implement the join operators of the relational algebra.

    ▪ Reduce side join: key = common attribute, value = concatenation of name of the table and content.                
                (userid=u1,[“User:name=Paolo,surname=Garza”, “Likes:movieGenre=horror”, “Likes:movieGenre=adventure”] will generate the following output (key,value pairs

            ▪ (userid=u1,“name=Paolo,surname=Garza,genre=horror”)

            ▪ (userid=u1,“name=Paolo,surname=Garza, genre=adventure”)

    
    ▪ Map side join(Map only job): one table is large the other is small enough to be contained to main memory (provided by D cache on setup). Same as before.

## Multiple inputs

    Reading data from multiple inputs with different formats --> one mapper per input dataset.

    MultipleInputs.addInputPath(job, new Path(args[1]), TextInputFormat.class, Mapper1.class);
    
    MultipleInputs.addInputPath(job, new Path(args[2]), TextInputFormat.class, Mapper2.class);

## Multiple outputs

    Stored in one single output directory with multiple output files with different prefixes.

    MultipleOutputs.addNamedOutput(job, "hightemp", TextOutputFormat.class, Text.class, NullWritable.class);
 
    MultipleOutputs.addNamedOutput(job, "normaltemp", TextOutputFormat.class, Text.class, NullWritable.class);

    There will be one output file of each type for each reducer.

    - Reducer or Mapper (if map-only job)

        private MultipleOutputs<Text, NullWritable> mos = null;
        
        //In setup of mapper or reducer

        mos = new MultipleOutputs<Text, NullWritable>(context);

        //method

        mos.write("hightemp", key, value);  // hightemp-

        //In cleanup of mapper/reducer

         mos.close();

## Distributed cache

A copy of the shared/cached (HDFS) files should be available locally in all nodes used to run the application. created only in 
the servers running the application that uses the shared file(s)


// Add the shared/cached HDFS file in the distributed cache

    job.addCacheFile(new Path("hdfspath/filename").toUri());

// Setup of mapper/reducer

    URI[] urisCachedFiles = context.getCacheFiles();

// In this example the content of the first shared file is opened.

    BufferedReader file = new BufferedReader(new FileReader(new File(new Path(urisCachedFiles[0].getPath()).getName())));

    while ((line = file.readLine()) != null) {
    
    // process the current line
    
    }


    file.close();
The local copy of the file is used by all mappers (reducers) running on the same node/server


## SQL operators
MapReduce implementation is efficient only when a full scan of the input table(s) is needed (no selective queries)

Tables can be stored in the HDFS distributed file system, broken in blocks and spread across the servers of the Hadoop cluster.
relations/tables do not contain duplicate records by definition, satisfied by both the input and the output relations/tables.

    - Selection: map only job
    - Projection: map selects the attribute (value = null), reducer removes the duplicates (value = null)
    - Union(same schema): map selects input record (value = null), reducer removes the duplicates (value = null)
    - Intesection(same schema): map selects input record (value = R or S, 2 mappers are needed), reducer emits only if both are present (value = null) 
    - Difference(same schema): map selects input record (value = R or S, 2 mappers are needed), reducer emits only if R is present (value = null)
    - Join 
    - Aggregation and Group by: using Summarization pattern
