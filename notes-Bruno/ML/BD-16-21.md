# ML

Spark MLlib is based on a set of basic local and distributed data types
    - Local vector
    - Labeled point
    - Local matrix
    - Distributed matrix

Estimators are algorithms or models that are used for training or fitting on data.

Transformers take in a dataset and perform transformations on it based on what they've learned during the training phase.

## Logic regression

LabeledPoint represents a single data point used for training in a supervised learning setting, where each data point has a feature vector and a label associated with it.

    JavaRDD<LabeledPoint> trainingRDD=trainingData.map(record -> 
{
    String[] fields = record.split(",");
    double classLabel = Double.parseDouble(fields[0]);
    double[] attributesValues = new double[3];
    attributesValues[0] = Double.parseDouble(fields[1]);
    attributesValues[1] = Double.parseDouble(fields[2]);
    attributesValues[2] = Double.parseDouble(fields[3]);
    Vector attrValues= Vectors.dense(attributesValues);
    return new LabeledPoint(classLabel, attrValues);
    });

    Dataset<Row> training = ss.createDataFrame(trainingRDD, LabeledPoint.class).cache();
    LogisticRegression lr = new LogisticRegression();
    lr.setMaxIter(10); //n iteration
    lr.setRegParam(0.01); //regularization parameter

    Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] {lr}); // sequence of transformers and estimators to apply to training data
    PipelineModel model = pipeline.fit(training);

    JavaRDD<LabeledPoint> unlabeledRDD=unlabeledData.map(record -> 
    {
        String[] fields = record.split(",");
        double[] attributesValues = new double[3];
 
        attributesValues[0] = Double.parseDouble(fields[1]); //removing -->,1,1,2
        attributesValues[1] = Double.parseDouble(fields[2]);
        attributesValues[2] = Double.parseDouble(fields[3]);

        Vector attrValues= Vectors.dense(attributesValues);

        double classLabel = -1;

        return new LabeledPoint(classLabel, attrValues);
        });

    Dataset<Row> test = ss.createDataFrame(unlabeledRDD, LabeledPoint.class);

    Dataset<Row> predictions = model.transform(test); //feautures, label, rawPrediction, probability, prediction

    Dataset<Row> predictionsDF=predictions.select("features", "prediction")

    JavaRDD<Row> predictionsRDD= predictionsDF.javaRDD();

    predictionsRDD.saveAsTextFile(outputPath)

## Decision Tree

DecisionTreeClassifier dc= new DecisionTreeClassifier();

dc.setImpurity("gini");

Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] {dc});

PipelineModel model = pipeline.fit(training);

//same as logical regression

## Estimator 
The Estimators StringIndexer and IndexToString support the transformation of categorical class label into numerical one

@SuppressWarnings("serial") //for debubbing

JavaRDD<MyLabeledPoint> trainingRDD=trainingData.map(record -> 
{
        String[] fields = record.split(",");
        String classLabel = fields[0];
        double[] attributesValues = new double[3];
        
        attributesValues[0] = Double.parseDouble(fields[1]);
        attributesValues[1] = Double.parseDouble(fields[2]);
        attributesValues[2] = Double.parseDouble(fields[3]);
        Vector attrValues= Vectors.dense(attributesValues);

        return new MyLabeledPoint(classLabel, attrValues);
        });

Dataset<Row> training = 
ss.createDataFrame(trainingRDD, MyLabeledPoint.class).cache();

// The StringIndexer Estimator is used to map each class label 
// value to an integer value (casted to a double)
StringIndexerModel labelIndexer = new StringIndexer()
 .setInputCol("categoricalLabel")
 .setOutputCol("label")
 .fit(training);

 DecisionTreeClassifier dc= new DecisionTreeClassifier();
 dc.setImpurity("gini");

// IndexToString creates a new column (called predictedLabel in 
// this example) that is based on the content of the prediction column
IndexToString labelConverter = new IndexToString()
 .setInputCol("prediction")
 .setOutputCol("predictedLabel")
 .setLabels(labelIndexer.labels());

Pipeline pipeline = new Pipeline()
 .setStages(new PipelineStage[] {labelIndexer,dc,labelConverter});
PipelineModel model = pipeline.fit(training);

JavaRDD<String> unlabeledData=sc.textFile(inputFileTest);
JavaRDD<MyLabeledPoint> unlabeledRDD=unlabeledData.map(record -> 
{
    String[] fields = record.split(",");

    double[] attributesValues = new double[3];
    
    attributesValues[0] = Double.parseDouble(fields[1]);
    attributesValues[1] = Double.parseDouble(fields[2]);
    attributesValues[2] = Double.parseDouble(fields[3]);

    Vector attrValues= Vectors.dense(attributesValues);
    // The class label in unknown.
    // To create a MyLabeledPoint a categoricalLabel value must be 
    // specified also for the unlabeled data. 
    // I set it to "Positive" (we must set the value of the class label to 
    // a valid value otherwise IndexToString raises an error). 
    // The specified value does not impact on the prediction because 
    // the categoricalLabel column is not used to perform the 
    // prediction.
    String classLabel = new String("Positive");
    return new MyLabeledPoint(classLabel, attrValues);
});

Dataset<Row> unlabeled = 
ss.createDataFrame(unlabeledRDD, MyLabeledPoint.class);

Dataset<Row> predictions = model.transform(unlabeled);

Dataset<Row> predictionsDF=
predictions.select("features", "predictedLabel");
JavaRDD<Row> predictionsRDD= predictionsDF.javaRDD();
predictionsRDD.saveAsTextFile(outputPath);

sc.close();



## Sparse data 

LIBSVM --> It is a commonly used textual format to represent sparse documents/data points

label index1:value1 index2:value2 //label:integer with class label

Features = [4.1, 0 , 2.5, 1.2] -- Label = 0

0 1:4.1 3:2.5 4:1.2

Dataset<Row> data = ss.read().format("libsvm").load("sample_libsvm_data.txt");


## Textual data classification

Each line contains one document and its class
    
    ▪ The class label

    ▪ A list of words (the text of the document)

Create a classification model based on the logistic regression algorithm for textual documents

    1. the textual part of the input data must be translated in a set of attributes in order to represent the data as a table (tokenization)
    2. Many words are useless (stopword removal)

The words appearing in few documents allow distinguish the content of those documents (and hence the class label) with respect to the others

    3. Traditionally a weight, based on the TF-IDF measure(Term Frequency-Inverse Document Frequency), is used to assign a difference importance to the words based on their frequency in the collection (TF-IDF computation)

after the pre-processing transformations must contain, as usual, the columns label, features

JavaRDD<LabeledDocument> trainingRDD=trainingData.map(record -> {
    String[] fields = record.split(",");
    double classLabel = Double.parseDouble(fields[0]);
    String text = fields[1];
    return new LabeledDocument(classLabel, text);
    });


    // Configure an ML pipeline, which consists of five stages: 
    // tokenizer -> split sentences in set of words
    // remover -> remove stopwords
    // hashingTF -> map set of words to a fixed-length feature vectors (each 
    // word becomes a feature and the value of the feature is the frequency of
    // the word in the sentence)
    // idf -> compute the idf component of the TF-IDF measure
    // lr -> logistic regression classification algorithm

    Tokenizertokenizer = new Tokenizer()
    .setInputCol("text")
    .setOutputCol("words");

    StopWordsRemoverremover = new StopWordsRemover()
    .setInputCol("words")
    .setOutputCol("filteredWords"); 

    HashingTF hashingTF = new HashingTF()
    .setNumFeatures(1000)
    .setInputCol("filteredWords")
    .setOutputCol("rawFeatures"); 

    IDF idf = new IDF()
    .setInputCol("rawFeatures")
    .setOutputCol("features");

    LogisticRegressionlr = new LogisticRegression()
    .setMaxIter(10)
    .setRegParam(0.01);

Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] {tokenizer, remover, hashingTF, idf, lr});

PipelineModel model = pipeline.fit(training);

Dataset<Row> predictions = model.transform(unlabeled);


##  Parameter tuning
Textual Classification and Parameter Tuning in Spark MLlib

## Textual Classification and Parameter Tuning in Spark MLlib

### Overview
- Setting algorithm parameters is challenging, often done via brute-force approach to optimize quality indices.
- Cross-validation (CV) helps overcome bias in parameter tuning, evaluating models on multiple splits.

### Grid-based Approach in Spark
- Spark supports a grid-based evaluation of parameter settings using MLlib pipelines.
- **Input**:
  - MLlib pipeline.
  - Set of parameter values for evaluation.
  - Quality evaluation metric.
- **Output**:
  - Model with the best parameter setting in terms of quality metric.

### Example - Tuning Logistic Regression
- Tuning a logistic regression classifier using cross-validation on a structured dataset.
- **Parameters considered**:
  - Maximum iterations: 10, 100, 1000
  - Regularization parameter: 0.1, 0.01 (6 configurations: 3x2)
- Java code example demonstrates the process using Spark MLlib APIs.

### Java Code Explanation
- Code begins with setting SparkSession and SparkContext.

#### Training Step:
- Reads and preprocesses training data, creating LabeledPoints.
- Prepares training dataset and defines the logistic regression model.
- Constructs a grid of parameter values for cross-validation.
- Creates a CrossValidator instance, specifying the pipeline, parameter grids, evaluator, and fold count.
- Fits the CrossValidator on the training data, obtaining the best model.

#### Prediction Step:
- Reads and preprocesses test/unlabeled data.
- Uses the trained model to predict class labels for test data.
- Selects relevant features and predictions, saves results to an output file.
- Closes the Spark Context.


package it.polito.bigdata.spark.sparkmllib;

import org.apache.spark.api.java.*;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.ml.Pipeline;
import org.apache.spark.ml.PipelineStage;
import org.apache.spark.ml.classification.LogisticRegression;
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator;
import org.apache.spark.ml.linalg.Vector;
import org.apache.spark.ml.linalg.Vectors;
import org.apache.spark.ml.feature.LabeledPoint;
import org.apache.spark.ml.param.ParamMap;
import org.apache.spark.ml.tuning.CrossValidator;
import org.apache.spark.ml.tuning.CrossValidatorModel;
import org.apache.spark.ml.tuning.ParamGridBuilder;

public class SparkDriver {
    public static void main(String[] args) {
        String inputFileTraining;
        String inputFileTest;
        String outputPath;
        inputFileTraining = args[0];
        inputFileTest = args[1];
        outputPath = args[2];

        SparkSession ss = SparkSession.builder()
                .appName("MLlib - logistic regression - Cross Validation")
                .getOrCreate();

        JavaSparkContext sc = new JavaSparkContext(ss.sparkContext());

        // Read training data from a text file
        JavaRDD<String> trainingData = sc.textFile(inputFileTraining);

        // Map each input record to a LabeledPoint
        JavaRDD<LabeledPoint> trainingRDD = trainingData.map(record -> {
            String[] fields = record.split(",");
            double classLabel = Double.parseDouble(fields[0]);
            double[] attributesValues = new double[3];

            attributesValues[0] = Double.parseDouble(fields[1]);
            attributesValues[1] = Double.parseDouble(fields[2]);
            attributesValues[2] = Double.parseDouble(fields[3]);

            Vector attrValues = Vectors.dense(attributesValues);
            return new LabeledPoint(classLabel, attrValues);
        });

        // Prepare training data
        Dataset<Row> training = ss.createDataFrame(trainingRDD, LabeledPoint.class).cache();

        // Create a LogisticRegression object
        LogisticRegression lr = new LogisticRegression();

        // Define the pipeline
        Pipeline pipeline = new Pipeline().setStages(new PipelineStage[]{lr});

        // Define parameter grid for cross-validation
        ParamMap[] paramGrid = new ParamGridBuilder()
                .addGrid(lr.maxIter(), new int[]{10, 100, 1000})
                .addGrid(lr.regParam(), new double[]{0.1, 0.01})
                .build();

        // Create CrossValidator
        CrossValidator cv = new CrossValidator()
                .setEstimator(pipeline)
                .setEstimatorParamMaps(paramGrid)
                .setEvaluator(new BinaryClassificationEvaluator())
                .setNumFolds(3);

        // Fit CrossValidator to training data
        CrossValidatorModel model = cv.fit(training);

        // Read unlabeled data
        JavaRDD<String> unlabeledData = sc.textFile(inputFileTest);

        // Map each unlabeled input record to a LabeledPoint
        JavaRDD<LabeledPoint> unlabeledRDD = unlabeledData.map(record -> {
            String[] fields = record.split(",");
            double[] attributesValues = new double[3];

            attributesValues[0] = Double.parseDouble(fields[1]);
            attributesValues[1] = Double.parseDouble(fields[2]);
            attributesValues[2] = Double.parseDouble(fields[3]);

            Vector attrValues = Vectors.dense(attributesValues);
            double classLabel = -1;
            return new LabeledPoint(classLabel, attrValues);
        });

        // Prepare test data
        Dataset<Row> test = ss.createDataFrame(unlabeledRDD, LabeledPoint.class);

        // Make predictions on test data
        Dataset<Row> predictions = model.transform(test);

        // Select relevant features and predictions, save results to an output file
        Dataset<Row> predictionsDF = predictions.select("features", "prediction");
        JavaRDD<Row> predictionsRDD = predictionsDF.javaRDD();
        predictionsRDD.saveAsTextFile(outputPath);

        // Close Spark Context
        sc.close();
    }
}

## Spark MLlib Clustering 

- **Algorithms Provided**:
  - Spark MLlib provides several clustering algorithms:
    - K-means
    - Gaussian mixture
    - (Other unspecified algorithms)

- **Algorithm Characteristics**:
  - Each algorithm has specific parameters.
  - All provided algorithms identify groups/clusters and assign objects to single clusters.
  - They operate exclusively with numerical data; categorical values need mapping to numerical values.
  
- **Input Data**:
  - MLlib clustering algorithms require input as `Dataset<Row>` containing a column named `features`.
  - Data type for `features`: `org.apache.spark.ml.linalg.Vectors`.
  - Only the `features` column is considered; other columns are disregarded.

- **Example**:
  - Input data example: Customer profiles with attributes like `MonthlyIncome` and `NumChildren`.
  - Input training data is formatted as vectors within the `features` column.

- **K-means Specifics**:
  - K-means is a popular algorithm in clustering.
  - Key parameter: `K` (number of clusters), which requires careful selection.
  - Identifies spherical-shaped clusters only.

### Application Example:
- An example demonstrates applying the K-means algorithm using Spark's MLlib.
- Input dataset comprises fixed numerical attributes.
- Example code assumes pre-normalized data within the range of [0-1].

### Code Snippets:
- The provided code demonstrates the implementation of K-means using Spark's MLlib.
- It involves steps like reading input data, mapping records to vectors, creating DataFrames, defining K-means, and executing clustering.


package it.polito.bigdata.spark.sparkmllib;

import java.io.Serializable;
import org.apache.spark.ml.Pipeline;
import org.apache.spark.ml.PipelineModel;
import org.apache.spark.ml.PipelineStage;
import org.apache.spark.ml.clustering.KMeans;
import org.apache.spark.ml.linalg.Vector;
import org.apache.spark.ml.linalg.Vectors;
import org.apache.spark.api.java.*;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;


public class InputRecord implements Serializable
{
private Vector features;
public Vector getFeatures() {
return features;
}
public void setFeatures(Vector features) {
this.features = features;
}
public InputRecord(Vector features) {
this.features = features;
}
}

public class SparkDriver {
    public static void main(String[] args) {
        String inputFile;
        String outputPath;
        inputFile = args[0];
        outputPath = args[1];
        
        // Create a Spark Session object and set the name of the application
        SparkSession ss = SparkSession.builder()
                .appName("MLlib - K-means").getOrCreate();
        
        // Create a Java Spark Context from the Spark Session
        JavaSparkContext sc = new JavaSparkContext(ss.sparkContext());
        
        // Read training data from a textual file
        JavaRDD<String> inputData = sc.textFile(inputFile);
        
        // Map each input record/data point of the input file to a InputRecord object
        JavaRDD<InputRecord> inputRDD = inputData.map(record -> {
            String[] fields = record.split(",");
            double[] attributesValues = new double[3];
            attributesValues[0] = Double.parseDouble(fields[0]);
            attributesValues[1] = Double.parseDouble(fields[1]);
            attributesValues[2] = Double.parseDouble(fields[2]);
            Vector attrValues = Vectors.dense(attributesValues);
            return new InputRecord(attrValues);
        });
        
        // Create a DataFrame based on the input data
        Dataset<Row> data = ss.createDataFrame(inputRDD, InputRecord.class).cache();
        
        // Create a k-means object
        KMeans km = new KMeans();
        km.setK(2); // Set the value of k (number of clusters)
        
        // Define the pipeline that is used to cluster the input data
        Pipeline pipeline = new Pipeline().setStages(new PipelineStage[]{km});
        
        // Execute the pipeline on the data to build the clustering model
        PipelineModel model = pipeline.fit(data);
        
        // Apply the clustering model on the data to assign them to clusters
        Dataset<Row> clusteredData = model.transform(data);
        
        // Save the result in an HDFS file
        JavaRDD<Row> clusteredDataRDD = clusteredData.javaRDD();
        clusteredDataRDD.saveAsTextFile(outputPath);
        
        // Close the Spark Context object
        sc.close();
    }
}

### Spark MLlib Itemset and Association Rule Mining

Minimum Frequency (Min Support):

Minimum frequency refers to the threshold for the occurrence of an itemset in the dataset.

Minimum Confidence:

Minimum confidence is a measure of the strength of association between items in an association rule.


An association rule is a pattern-based relationship or correlation between different items or variables within a dataset

{Milk, Bread} => {Butter}
- Antecedent: Customers who bought Milk and Bread.
- Consequent: High probability of also buying Butter.
- Support: Percentage of transactions containing {Milk, Bread, Butter}.
- Confidence: Probability of buying Butter given that Milk and Bread are bought together.

- **Algorithms Provided**:
  - FP-growth algorithm for itemset mining.
    - Extracts sets of items with minimum frequency.
  - Rule mining algorithm.
    - Extracts association rules with minimum frequency and confidence.
    - Rules with one single item in the consequent are extracted.

- **Input Data**:
  - Transactional dataset: Each transaction defined as a set of items.
  - Example: 
    ```
    A B C D
    A B
    B C
    A D E
    ```
  - FP-growth requires a transactional dataset as input.

- **FP-growth**:
  - Popular and efficient itemset mining algorithm.
  - Characterized by a minimum support threshold (minsup).
  - Uses the minsup threshold to limit the mined itemsets.

- **Association Rule Mining**:
  - Mined based on minsup and minimum confidence threshold (minconf).
  - Minsup specified during itemset mining, minconf during rule mining.

- **MLlib Implementation**:
  - Based on DataFrames.
  - FP-growth not invoked via pipelines.

- **Mining Steps**:
  - Instantiate FP-Growth object, set parameters.
  - Invoke `fit(input data)` method.
  - Retrieve frequent itemsets and association rules using methods.

- **Input Format**:
  - Input of itemset and rule mining: `Dataset<Row>` with a column called `items`.
  - Each record contains one transaction, represented as `java.util.List<String>`.

- **Example Input Data**:
A B C D
A B
B C
A D E

- **Code Example**:
- Uses Java with Spark's MLlib.
- Involves reading input data, mapping to `Transaction` objects, creating DataFrames, configuring FP-growth, extracting itemsets and rules, and saving results in output folders.
package it.polito.bigdata.spark.sparkmllib;

import java.io.Serializable;
import java.util.Arrays;
import java.util.List;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.ml.fpm.FPGrowth;
import org.apache.spark.ml.fpm.FPGrowthModel;

public class SparkDriver {
    public static void main(String[] args) {
        String inputFile;
        String outputFolderItemsets;
        String outputFolderRules;
        double minSupport;
        double minConfidence;
        
        inputFile = args[0];
        outputFolderItemsets = args[1];
        outputFolderRules = args[2];
        minSupport = Double.parseDouble(args[3]);
        minConfidence = Double.parseDouble(args[4]);
        
        // Create a Spark Session object
        SparkSession ss = SparkSession.builder()
                .appName("MLlib - Itemset and Association rule mining").getOrCreate();
        JavaSparkContext sc = new JavaSparkContext(ss.sparkContext());

        // Read input data
        JavaRDD<String> inputData = sc.textFile(inputFile);
        JavaRDD<Transaction> inputRDD = inputData.map(line -> {
            String[] items = line.split(" ");
            return new Transaction(Arrays.asList(items));
        });

        // Create DataFrame based on input data
        Dataset<Row> transactionsData = ss.createDataFrame(inputRDD, Transaction.class).cache();

        // Create an FPGrowth object
        FPGrowth fp = new FPGrowth();
        fp.setMinSupport(minSupport)
                .setMinConfidence(minConfidence);

        // Extract frequent itemsets and association rules
        FPGrowthModel itemsetsAndRulesModel = fp.fit(transactionsData);
        Dataset<Row> frequentItemsets = itemsetsAndRulesModel.freqItemsets();
        Dataset<Row> frequentRules = itemsetsAndRulesModel.associationRules();

        // Save itemsets and rules in HDFS output folders
        JavaRDD<Row> itemsetsRDD = frequentItemsets.javaRDD();
        JavaRDD<Row> rulesRDD = frequentRules.javaRDD();
        itemsetsRDD.saveAsTextFile(outputFolderItemsets);
        rulesRDD.saveAsTextFile(outputFolderRules);

        // Close Spark Context and stop Spark Session
        sc.close();
        ss.stop();
    }
}

// Transaction class
class Transaction implements Serializable {
    private List<String> items;

    public List<String> getItems() {
        return items;
    }

    public void setItems(List<String> items) {
        this.items = items;
    }

    public Transaction(List<String> items) {
        this.items = items;
    }
}

