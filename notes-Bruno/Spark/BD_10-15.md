# Spark

## Introduction to Spark
    - Generality: diverse workloads, operators, job sizes
    - Low latency: sub-second
    - Fault tolerance: faults are the norm, not the exception
    - Simplicity: often comes from generality

Iterative jobs, with MapReduce, involve a lot of disk I/O for each iteration and stage
Spark-->Keep more data in main memory, Data are shared between the iterations by using the main memory.  Data are read only once from HDFS and stored 
in main memory

Data are represented as Resilient Distributed Datasets (RDDs)
    - Partitioned/Distributed collections of objects spread across the nodes of a cluster
    - Stored in main memory (when it is possible) or on local disk

RDDs are built and manipulated through a set of parallel 
    - Transformations 
    ▪ map, filter, join, …
    - Actions
    ▪ count, collect, save, …

RDDs are automatically rebuilt on machine failure

RDDs (immutable) are stored in the main memory of the executors running in the nodes of the cluster (when it is possible) or in the local disk of the nodes if there is not enough main memory.

Allow executing in parallel the code invoked on them

▪ Each executor of a worker node runs the specified code on its partition of the RDD


Spark vs MapReduce
 Lower overhead for starting jobs
 Less expensive shuffles

The Driver program. Contains the main method. “Defines” the workflow of the application. 

Accesses Spark through the SparkContext object

▪ The SparkContext object represents a connection to the cluster

Defines Resilient Distributed Datasets (RDDs) that are “allocated” in the nodes of the cluster

▪ Invokes parallel operations on RDDs

The SparkContext object allows 
▪ Creating RDDs
▪ “Submitting” executors (processes) that execute in parallel specific operations on RDDs

Each executor runs on its partition of the RDD(s) the operations that are specified in the driver

Task :A unit of work that will be sent to one executor

Job: A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect)


Each job gets divided into smaller sets of tasks called stages. The output of one stage is the input of the next stage(s).
The shuffle operation is always executed between two stages


spark-submit --class it.polito.spark.DriverMyApplication --deploy-mode cluster --master yarn MyApplication.jar arguments


--class spark application we want to execute
--master environment/scheduler used to execute the application (where)
--deploy-mode where the driver is launched


## RDD-based programming

JavaRDD<String> lines=sc.textFile(inputFile)

The data is lazily read from the input file only when the data is needed 

List<String> inputList = Arrays.asList("First element","Second element", "Third element");

JavaRDD<String> distList = sc.parallelize(inputList);

lines.saveAsTextFile(outputPath);

Spark computes the content associated with an RDD only when the content is needed

List<String> contentOfLines=lines.collect();

Operations

    - Transformations (return a new RDD,lazily)
    - Actions (return a local variable writing to Driver or storage)


Graph of dependencies between RDDs represents the information about which RDDs are used to create a new RDD (lineage graph)


Transformation

    JavaRDD<Integer> greaterRDD = 
    inputRDD.filter(element -> { if (element>2)
                                    return true; 
                                else
                                    return false; 
                                });

    JavaRDD<Integer> squaresRDD= inputRDD.map(element -> new Integer(element*element));

    JavaRDD<String> listOfWordsRDD= inputRDD.flatMap(x -> Arrays.asList(x.split(" ")).iterator());

    JavaRDD<String> distinctNamesRDD= inputRDD.distinct()

    JavaRDD<String> randomSentencesRDD= inputRDD.sample(false,0.2)

    JavaRDD<Integer> outputUnionRDD= inputRDD1.union(inputRDD2);

    JavaRDD<Integer> outputIntersectionRDD= inputRDD1.intersection(inputRDD2);

    JavaRDD<Integer> outputSubtractRDD= inputRDD1.subtract(inputRDD2);

    JavaPairRDD<Integer, Integer> outputCartesianRDD= inputRDD1.cartesian(inputRDD2);

Actions

    List<Integer> retrievedValues = inputRDD.collect();

    long numLinesDoc2 = inputRDD2.count();

    java.util.Map<String, java.lang.Long> namesOccurrences = namesRDD.countByValue();
   
The countByValue action returns a local Java Map object containing the information about the number of times each element occurs in the RDD

    List<Integer> retrievedValues = inputRDD.take(2); //Return list

    Integer retrievedValue = inputRDD.first(); //Return first

    List<Integer> retrievedValues = inputRDD.top(2); //take the biggest

    List<Integer> retrievedValues = inputRDD.top(2, new MyComparatorTop());

    public class MyComparatorTop implements Comparator<Integer>, Serializable {

        private static final long serialVersionUID= 1L;
    
    @Override
    
        public int compare(Integer value1, Integer value2) {
    
        // Compare the numbers and return first the smallest one
    
        return -1*(value1.compareTo(value2));
    
        }
    
    }

    List<Integer> retrievedValues = inputRDD.takeOrdered(2); // take the smallest

    List<Integer> randomValues= inputRDD.takeSample(false, 2);

    Integer sum= inputRDDReduce.reduce((element1, element2) -> element1+element2);//commutative associative

    T fold(T zeroValue, Function2<T, T, T> f) must be associative but not commutative

    U aggregate(U zeroValue, Function2<U,T,U> seqOp, Function2<U,U,U> combOp) must be associative and result can be different class than input

    SumCountresult = inputRDDAggr.aggregate(zeroValue, 
        (a, e) -> {

        a.sum = a.sum + e;
        
        a.numElements = a.numElements + 1;
        
        return a;
        
        }, 
        
        (a1, a2) -> {
        
        a1.sum = a1. sum + a2.sum;
        
        a1.numElements = a1.numElements + a2.numElements;
        
        return a1;
        
        } );


JavaPairRDD<String, Integer> nameOneRDD= namesRDD.mapToPair(name -> new Tuple2<String, Integer>(name, 1));

JavaPairRDD<String, Integer> wordOneRDD= linesRDD.flatMapToPair(line -> {
    
        List<Tuple2<String, Integer>> pairs = new ArrayList<>();

        String[] words = line.split(" ");

        for (String word : words) {

        pairs.add(new Tuple2<String, Integer>(word, 1));

        }

        return pairs.iterator();

    });

ArrayList<Tuple2<String, Integer>> nameAge = new ArrayList<Tuple2<String, Integer>>();

JavaPairRDD<String, Integer> nameAgeRDD = sc.parallelizePairs(nameAge);

Transformations

        JavaPairRDD<String, Integer> youngestPairRDD= nameAgeRDD.reduceByKey( //associative and commutative
                    (age1, age2) -> {
                        
                        if (age1<age2)
                    
                            return age1;
                    
                        else
                        
                            return age2;
                    
                    });
        
        JavaPairRDD<K,V> foldByKey(V zeroValue, Function2<V,V,V> f)  //associative but could also be not commutative

        JavaPairRDD<String, AvgCount> avgAgePerNamePairRDD=nameAgeRDD.combineByKey( //associative, create combiner mergevalue merge combiner
                        
                        inputElement-> new AvgCount(inputElement, 1),

                        (intermediateElement, inputElement) -> {
                        
                            AvgCount combine=new AvgCount(inputElement, 1);
                            
                            combine.total=combine.total+intermediateElement.total;
                            
                            combine.numValues = combine.numValues + intermediateElement.numValues;
                            
                            return combine; 
                        
                        }, 
                        
                        (intermediateElement1, intermediateElement2) -> {
                        
                            AvgCount combine = new AvgCount(intermediateElement1.total, intermediateElement1.numValues);
                            
                            combine.total=combine.total+intermediateElement2.total;
                            
                            combine.numValues=combine.numValues + intermediateElement2.numValues;
                            
                            return combine; 
                        
                        });

groupByKey is useful if you need to apply an aggregation/compute a function that is not associative

Transformation

        JavaPairRDD<String, Integer> nameAgePlusOneRDD= nameAgeRDD.mapValues(age -> new Integer(age+1));

        JavaPairRDD<K,U> flatMapValues(Function<V, Iterable<U>> f)

        JavaRDD<K> keys();

        JavaRDD<V> values();

        JavaPairRDD<String, Integer> sortedNameAgeRDD=nameAgeRDD.sortByKey(); 

        JavaPairRDD<String, Integer> selectedUsersPairRDD = profilesPairRDD.subtractByKey(bannedPairRDD);

        JavaPairRDD<Integer, Tuple2<String, String>> joinPairRDD =questionsPairRDD.join(answersPairRDD);

        JavaPairRDD<Integer, Tuple2<Iterable<String>, Iterable<String>>> cogroupPairRDD = moviesPairRDD.cogroup(directorsPairRDD); //(1, (["Star Trek", "Forrest Gump"], ["Woody Allen"]) )

Actions

        java.util.Map<String, java.lang.Object> movieNumRatings = movieRatingRDD.countByKey();

        java.util.Map<String, String> retrievedPairs = usersRDD.collectAsMap();

        java.util.List<Integer> movieRatings = movieRatingRDD.lookup("Forrest Gump");


Spark provides specific actions for a specific numerical type of RDD called JavaDoubleRDD

Transformation

JavaDoubleRDD parallelizeDoubles(java.util.List<Double> list)

JavaDoubleRDDlenghtsDoubleRDD= surnamesRDD.mapToDouble(surname -> (double)surname.length());

JavaDoubleRDDwordLenghtsDoubleRDD= sentencesRDD.flatMapToDouble(sentence -> 
                                    
                                    {

                                        String[] words=sentence.split(" ");

                                        // Compute the length of each word

                                        ArrayList<Double> lengths=new ArrayList<Double>(); 

                                        for (String word: words) { 

                                        lengths.add(new Double(word.length()));

                                        } 

                                        return lengths.iterator(); 

                                    });


Actions
List<Double> inputList = Arrays.asList(1.5, 3.5, 2.0);
// Build a DoubleRDDfrom the local list
JavaDoubleRDDdistList = sc.parallelizeDoubles(inputList);

System.out.println("sum: "+distList.sum());
System.out.println("mean: "+distList.mean());
System.out.println("stdev: "+distList.stdev());
System.out.println("variance: "+distList.variance());
System.out.println("max: "+distList.max());
System.out.println("min: "+distList.min());



Spark computes the content of an RDD each time an ****action**** is invoked on it. If the same RDD is used multiple times in an application, Spark recomputes its content every time an actionis invoked on the RDD, or on one of its “descendants”.This is expensive, especially for iterative applications
 We can ask Spark to persist/cache RDDs

JavaRDD<T> persist(StorageLevel level)--> RDDTMP.persist();

JavaRDD<T> cache() --> RDDTMP.cache();--> MEMORY_ONLY

StorageLevel.MEMORY_ONLY
MEMORY_AND_DISK 
MEMORY_ONLY_SER
MEMORY_AND_DISK_SER
DISK_ONLY
MEMORY_ONLY_2, 
MEMORY_AND_DISK_2,
OFF_HEAP

Spark drop cache in least-recently-used (LRU) fashion

JavaRDD<T> unpersist()

Accumulator

 the scala SparkContext must be used 

instead of the JavaSparkContext

the values of the accumulators are correct you must update them in transformations or actions that are executed only one time in your application
    val data = sc.parallelize(Seq(1, 2, 3, 4, 5))
    data.foreach { num =>
      sumAccumulator.add(num)
    }
 
 org.apache.spark.SparkContext ssc = sc.sc();

final LongAccumulator invalidEmails = sc.sc().longAccumulator();
emailsRDD.foreach(line -> {
if (line.contains("@") == false)
invalidEmails.add(1);
 });
+invalidEmails.value()


Personalized accumulators
org.apache.spark.util.AccumulatorV2<T,T>
 Several methods must be implemented
▪ abstract void add(T value)
▪ abstract T value()
▪ abstract AccumulatorV2<T,T> copy()
MyAcculumator myAcc = new MyAccumulator();
sc.sc().register(myAcc, "MyNewAcc");

A broadcast variable is a read-only (medium/large) shared variable
A copy each “standard” variable is sent to all the tasks executing a Spark action using that variable
HashMap<String, Integer> dictionary=new HashMap<String, Integer>();
// Create a broadcast variable based on the content of dictionaryRDD
// Pay attention that a broadcast variable can be instantiated only
// by passing as parameter a local java variable and not an RDD.
// Hence, the collect method is used to retrieve the content of the 
// RDD and store it in the dictionary HashMap<String, Integer> variable
for (Tuple2<String, Integer> pair: dictionaryRDD.collect()) {
dictionary.put(pair._1(), pair._2());
}
final Broadcast<HashMap<String, Integer>> dictionaryBroadcast = 
sc.broadcast(dictionary);
dictionaryBroadcast.value().get(words[i]);



SPARK SQL

Spark SQL is the Spark component for structured data processing
 It provides a programming abstraction called Dataset and can act as a distributed SQL query engine

DataFrame
 A “particular” Dataset organized into named columns

A DataFrame is simply a Dataset of Row objects ; Dataset<Row>

All the Spark SQL functionalities based on

SparkSession ss =
SparkSession.builder().appName("App.Name").getOrCreate(); 

ss.stop();

Dataset<Row> load(String path)


 DataFrameReader dfr=ss.read().format("csv").option("header", 
true).option("inferSchema", true); // .option("multiline", true);

JavaRDD<Row> javaRDD() //JavaRDD of Row objects

java.lang.Object getAs(String columnName)

int fieldIndex(String columnName)

String getString(int position)

double getDouble(int position)

Datasets are more efficient than RDDs

must be JavaBean-compliant associated with a JavaBean must implement Serializable

 All its attributes/variables must have public setter and getter methods

 All its attributes/variables should be private


Dataset<T> createDataset(java.util.List<T> data, Encoder<T> encoder)

Encoder<Person> personEncoder = Encoders.bean(Person.class);
Encoder<Integer> Encoders.INT()
Encoder<String> Encoders.STRING()

// Define a Dataset of Person objects from the df DataFrame
Dataset<Person> ds = df.as(personEncoder);

Dataset<Person> personDS= ss.createDataset(persons, personEncoder);

Dataset<T> createDataset(RDD<T> inRDD, Encoder<T> encoder)

 inRDD is the input RDD to be converted in a Dataset
 Pay attention that the first parameter is a scala RDD and not a JavaRDD
▪ Use JavaRDD.toRDD(inJavaRDD) to convert a JavaRDD into a scala RDD

ds.show(2);

void printSchema() // the name of the attributes of the data stored in the Dataset

ds.count();

Dataset<Row> dfNamesAges = ds.select("name","age");

Dataset<Row> df = 
 ds.selectExpr("name", "age", "gender", "age+1 as newAge");  can generate errors at runtime if there are typos in the expressions

 Dataset<PersonNewAge> ds2 = ds.map(p -> { //checks also partially the “semantic” of the query at compile time
            PersonNewAge newPersonNA= 
            new PersonNewAge();
            newPersonNA.setName(p.getName());
            newPersonNA.setAge(p.getAge());
            newPersonNA.setGender(p.getGender());
            newPersonNA.setNewAge(p.getAge()+1);
                return newPersonNA; }
 , Encoders.bean(PersonNewAge.class)); // for the return obj 

FILTER

 Dataset<Person> dsSelected= ds.filter("age>=20 and age<=31"); // can generate errors at runtime ;alias where


FILTER with lambda func

Dataset<Person> dsSelected= ds.filter(p -> {
                                if (p.getAge()>=20 && p.getAge()<=31) 
                                    return true;
                                else 
                                    return false;
                                });

Dataset<Row> dfPersonLikes= dsPersons.join(dsUidSports, dsPersons.col("uid").equalTo(dsUidSports.col("uid")));


Dataset<Row> nonBannedProfiles= dsProfiles.join(dsBanned,dsProfiles.col("uid").equalTo(dsBanned.col("uid")),"leftanti");

Aggregate

Dataset<Row> averageAge = ds.agg(avg("age"), count(*));

// Group data by name
RelationalGroupedDatasetrgd=ds.groupBy("name");
// Compute the average of age for each group
Dataset<Row> nameAverageAge = rgd.avg("age")


Dataset<Person> sortedAgeName = 
ds.sort(new Column("age").desc(), new Column("name"));

// Assign the “table name” people to the ds Dataset
ds.createOrReplaceTempView("people");
// Select the persons with age between 20 and 31
// by querying the people table
Dataset<Row> selectedPersons = ss.sql("SELECT * FROM people WHERE age>=20 and age<=31");

Dataset<Row> dfPersonLikes= ss.sql("SELECT * from people, liked where people.uid=liked.uid");

Dataset<Row> nameAvgAgeCount = ss.sql("SELECT name, avg(age), count(name) FROM people GROUP BY name");


Dataset -save

Dataset<Person> ds =ss.read().format("csv").option("header", true)
.option("inferSchema", true).load("persons.csv").as(personEncoder);
// Save the file on the disk by using the CSV format
ds.write().format("csv").option("header", true).save(outputPath);


udf().register(String name, UDF function, DataTypedatatype) // returned value
ss.udf().register("length", (String name) -> name.length(), DataTypes.IntegerType);

// Usage in selectExpr
Dataset<Row> result=inputDF.selectExpr("length(name) as size");
//usage in SQL query
Dataset<Row> result=ss.sql("SELECT length(name) FROM profiles");